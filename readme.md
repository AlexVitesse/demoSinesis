# üß† RAG Expert Assistant

**RAG Expert Assistant** es una aplicaci√≥n avanzada construida con arquitectura modular en Python que convierte documentos en asistentes expertos usando **Retrieval-Augmented Generation (RAG)**. Carga documentos de cualquier tema (PDF, TXT, DOCX, etc.) y obt√©n respuestas contextualizadas como si interactuaras con un experto en el contenido.

---

## üöÄ Caracter√≠sticas Principales

- üìÑ **Carga m√∫ltiple de documentos** (PDF, TXT, DOCX, MD, etc.)
- üì¶ **Almacenamiento vectorial inteligente** con ChromaDB
- üß† **Embeddings optimizados** con modelos Hugging Face
- üîç **Recuperaci√≥n h√≠brida avanzada**: BM25 + Embeddings vectoriales
- üí¨ **Chat interactivo contextual** con IA conversacional
- üß± **Arquitectura modular escalable** con componentes desacoplados
- üìä **Estad√≠sticas en tiempo real** de documentos y embeddings
- üéõÔ∏è **Interfaz moderna** con Streamlit y componentes modulares
- üîß **Gesti√≥n inteligente de memoria** y optimizaci√≥n de recursos

---

## üèóÔ∏è Arquitectura del Sistema

### Componentes Principales

- **`UserInterface`**: Orquestador principal de la aplicaci√≥n
- **`VectorStoreManager`**: Gesti√≥n del almacenamiento vectorial
- **`FileManager`**: Procesamiento y carga de documentos
- **`DocumentUI`**: Interfaz de gesti√≥n de documentos
- **`DocumentDB`**: Persistencia de metadatos con SQLite
- **`RAGSystem`**: Motor principal de recuperaci√≥n y generaci√≥n

### M√≥dulos UI Especializados

```
ui_components/
‚îú‚îÄ‚îÄ model_manager.py          # Gesti√≥n de modelos de embeddings
‚îú‚îÄ‚îÄ sidebar.py                # Navegaci√≥n y configuraci√≥n
‚îú‚îÄ‚îÄ file_upload.py            # Carga de archivos avanzada
‚îú‚îÄ‚îÄ search_interface.py       # B√∫squeda sem√°ntica y por palabras clave
‚îî‚îÄ‚îÄ LLM/
    ‚îî‚îÄ‚îÄ llm_interface.py      # Chat inteligente contextual
```

---

## üõ†Ô∏è Stack Tecnol√≥gico

| Categor√≠a | Tecnolog√≠a | Prop√≥sito |
|-----------|------------|-----------|
| **Frontend** | Streamlit | Interfaz web interactiva |
| **Base de Datos Vectorial** | ChromaDB | Almacenamiento de embeddings |
| **Procesamiento LLM** | LangChain + Groq | Cadenas de recuperaci√≥n y generaci√≥n |
| **Embeddings** | Hugging Face Transformers | Modelos de embeddings sem√°nticos |
| **B√∫squeda H√≠brida** | BM25 + Vector Search | Recuperaci√≥n precisa de contexto |
| **Base de Datos** | SQLite | Metadatos y estado de documentos |
| **Procesamiento** | PyPDF2, python-docx | Extracci√≥n de texto de documentos |

---

## üìã Requisitos del Sistema

- **Python**: 3.9+ (recomendado 3.10+)
- **RAM**: M√≠nimo 4GB (recomendado 8GB+)
- **GPU**: Opcional (acelera embeddings)
- **Espacio**: 2GB+ libre para modelos y datos

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/tuusuario/demoSinesis.git
cd demoSinesis
```

### 2. Crear Entorno Virtual

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Variables de Entorno

Crea un archivo `.env` en la ra√≠z del proyecto:

```env
# API Keys (elige uno o varios)
GROQ_API_KEY=tu_clave_groq_aqui
OPENAI_API_KEY=tu_clave_openai_aqui

# Configuraci√≥n de Base de Datos
CHROMA_DB_PATH=BD/chroma_db_dir
SQLITE_DB_PATH=BD/documents.db

# Configuraci√≥n de Modelos
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
LLM_MODEL=llama3-8b-8192
LLM_TEMPERATURE=0.7
```

### 5. Crear Estructura de Directorios

```bash
mkdir -p BD/chroma_db_dir
mkdir -p temp_uploads
```

---

## üéØ Uso de la Aplicaci√≥n

### Iniciar la Aplicaci√≥n

```bash
streamlit run main.py
```

La aplicaci√≥n se abrir√° en `http://localhost:8501`

### Flujo de Trabajo

1. **üì§ Carga de Documentos**
   - Usa el panel lateral para subir archivos
   - Soporta PDF, TXT, DOCX, MD
   - Visualiza el progreso de procesamiento

2. **üîç Procesamiento Inteligente**
   - El sistema genera embeddings autom√°ticamente
   - Crea √≠ndices vectoriales y BM25
   - Almacena metadatos en SQLite

3. **üí¨ Chat Contextual**
   - Haz preguntas en lenguaje natural
   - Obt√©n respuestas basadas en tus documentos
   - Ve las fuentes utilizadas para cada respuesta

4. **üìä Monitoreo**
   - Estad√≠sticas de documentos cargados
   - M√©tricas de rendimiento
   - Estado del sistema vectorial

---

## üìÅ Estructura del Proyecto

```
rag-expert-assistant/
‚îú‚îÄ‚îÄ main.py                     # üöÄ Punto de entrada principal
‚îú‚îÄ‚îÄ ui.py                       # üñ•Ô∏è Interfaz principal refactorizada
‚îú‚îÄ‚îÄ rag.py                      # üß† Motor RAG principal
‚îú‚îÄ‚îÄ config.py                   # ‚öôÔ∏è Configuraciones globales
‚îú‚îÄ‚îÄ requirements.txt            # üì¶ Dependencias Python
‚îú‚îÄ‚îÄ .env                        # üîê Variables de entorno
‚îú‚îÄ‚îÄ README.md                   # üìñ Documentaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ BD/                         # üíæ Bases de datos
‚îÇ   ‚îú‚îÄ‚îÄ chroma_db_dir/         # Almacenamiento vectorial
‚îÇ   ‚îî‚îÄ‚îÄ documents.db           # Metadatos SQLite
‚îÇ
‚îú‚îÄ‚îÄ temp_uploads/              # üìÅ Archivos temporales
‚îÇ
‚îú‚îÄ‚îÄ ui_components/             # üß© Componentes modulares
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model_manager.py       # Gesti√≥n de modelos
‚îÇ   ‚îú‚îÄ‚îÄ sidebar.py             # Barra lateral
‚îÇ   ‚îú‚îÄ‚îÄ file_upload.py         # Carga de archivos
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py  # Procesamiento de documentos
‚îÇ   ‚îú‚îÄ‚îÄ search_interface.py    # Interfaz de b√∫squeda
‚îÇ   ‚îî‚îÄ‚îÄ LLM/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ llm_interface.py   # Chat con IA
‚îÇ
‚îú‚îÄ‚îÄ file_manager.py            # üìÇ Gesti√≥n de archivos
‚îú‚îÄ‚îÄ document_ui.py             # üìÑ UI de documentos
‚îú‚îÄ‚îÄ document_processing.py     # üîÑ Procesamiento de documentos
‚îú‚îÄ‚îÄ document_db.py             # üóÑÔ∏è Base de datos de documentos
‚îî‚îÄ‚îÄ vector_store.py            # üîç Gesti√≥n del almac√©n vectorial
```

---

## üé™ Casos de Uso

### üè¢ Empresarial
- **Capacitaci√≥n interna**: Manuales t√©cnicos y pol√≠ticas
- **Soporte t√©cnico**: Bases de conocimiento inteligentes
- **Consultor√≠a**: An√°lisis r√°pido de documentos contractuales

### üéì Acad√©mico/Investigaci√≥n
- **Investigaci√≥n cient√≠fica**: An√°lisis de papers y publicaciones
- **Estudios jur√≠dicos**: Consultas a leyes y reglamentos
- **An√°lisis documental**: Procesamiento de grandes vol√∫menes de texto

### üë®‚Äçüíº Personal/Profesional
- **Gesti√≥n del conocimiento**: Organizaci√≥n de documentos personales
- **Aprendizaje**: Interacci√≥n con libros y materiales de estudio
- **An√°lisis de contenido**: Extracci√≥n de insights de documentos

---

## ‚ö° Caracter√≠sticas Avanzadas

### üîç B√∫squeda H√≠brida
- **B√∫squeda vectorial**: Similitud sem√°ntica avanzada
- **BM25**: B√∫squeda por palabras clave tradicional
- **Ensemble**: Combinaci√≥n ponderada de ambos m√©todos

### üß† IA Conversacional
- **Contexto din√°mico**: Respuestas basadas en documentos cargados
- **Fuentes citadas**: Transparencia en las respuestas
- **Memoria de conversaci√≥n**: Mantiene el contexto del chat

### üìä Anal√≠ticas
- **M√©tricas de rendimiento**: Tiempo de respuesta y precisi√≥n
- **Estad√≠sticas de uso**: Documentos m√°s consultados
- **Monitoreo de recursos**: Uso de memoria y CPU

---

## üîß Configuraci√≥n Avanzada

### Personalizaci√≥n de Modelos

```python
# En config.py
EMBEDDING_CONFIG = {
    "model_name": "sentence-transformers/all-MiniLM-L6-v2",
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "normalize_embeddings": True
}

LLM_CONFIG = {
    "model": "llama3-8b-8192",  # o "gpt-3.5-turbo"
    "temperature": 0.7,
    "max_tokens": 1024,
    "top_p": 0.9
}
```

### Optimizaci√≥n de Rendimiento

```python
# Configuraci√≥n para documentos grandes
CHUNK_CONFIG = {
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "separators": ["\n\n", "\n", ". ", " "]
}

# Configuraci√≥n de recuperaci√≥n
RETRIEVAL_CONFIG = {
    "vector_k": 8,
    "bm25_k": 8,
    "score_threshold": 0.3,
    "ensemble_weights": [0.6, 0.4]  # [vectorial, bm25]
}
```

---

## üêõ Soluci√≥n de Problemas

### Errores Comunes

#### `ModuleNotFoundError: No module named 'rag'`
```bash
# Verificar estructura de directorios
# Asegurar que existe ui_components/__init__.py
touch ui_components/__init__.py
touch ui_components/LLM/__init__.py
```

#### `ChromaDB collection not found`
```bash
# Limpiar y recrear base vectorial
rm -rf BD/chroma_db_dir
# Recargar documentos en la aplicaci√≥n
```

#### `GROQ_API_KEY not found`
```bash
# Verificar archivo .env
echo "GROQ_API_KEY=tu_clave_aqui" >> .env
```

### Logs y Depuraci√≥n

La aplicaci√≥n genera logs detallados. Para habilitarlos:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## üöß Roadmap y Mejoras Futuras

### Versi√≥n 2.0
- [ ] **Multi-usuario**: Sesiones separadas por usuario
- [ ] **API REST**: Integraci√≥n externa completa
- [ ] **Historial persistente**: Conversaciones guardadas
- [ ] **An√°lisis avanzado**: Dashboard de m√©tricas

### Versi√≥n 2.1
- [ ] **Modelos locales**: Soporte para LLMs sin internet
- [ ] **Vectorizaci√≥n incremental**: Actualizaci√≥n sin recrear
- [ ] **Integraci√≥n cloud**: AWS, GCP, Azure
- [ ] **Plugin system**: Extensiones personalizadas

---

## ü§ù Contribuciones

¬°Las contribuciones son bienvenidas! Por favor:

1. Fork del repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

### Gu√≠as de Contribuci√≥n
- Seguir PEP 8 para estilo de c√≥digo
- Agregar docstrings a funciones nuevas
- Incluir tests para nuevas funcionalidades
- Actualizar documentaci√≥n cuando sea necesario

---

## üìà M√©tricas y Rendimiento

### Benchmarks T√≠picos
- **Carga de documento PDF (10 p√°ginas)**: ~15-30 segundos
- **Generaci√≥n de respuesta**: ~2-5 segundos
- **B√∫squeda en 100 documentos**: ~200-500ms
- **Uso de memoria**: ~500MB-2GB (dependiendo del corpus)

### Optimizaciones Implementadas
- ‚úÖ Cache de embeddings con LRU
- ‚úÖ Lazy loading de modelos
- ‚úÖ Batch processing para documentos m√∫ltiples
- ‚úÖ Compresi√≥n de vectores con cuantizaci√≥n

---

## üìÑ Licencia

Este proyecto est√° bajo la **Licencia MIT**. Ver el archivo [LICENSE](LICENSE) para m√°s detalles.

```
MIT License

Copyright (c) 2025 RAG Expert Assistant

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...
```

---

## üë®‚Äçüíª Autor

**Alex** üöÄ  
üìç **Campeche, M√©xico**  
üõ†Ô∏è **Consultor TI | Especialista en Python & AI**  

### Con√©ctame
- üìß Email: [eric.vazquez.condor@gmail.com](mailto:eric.vazquez.condor@gmail.com)
- üíº LinkedIn: [Eric Vazquez](https://www.linkedin.com/in/eric-alejandro-vazquez-gongora-49342b148/l)
- üêô GitHub: [AlexVitesse](https://github.com/AlexVitesse)

---

## üôè Agradecimientos

- **LangChain**: Por el framework de LLM
- **ChromaDB**: Por la base de datos vectorial
- **Streamlit**: Por la interfaz web intuitiva  
- **Hugging Face**: Por los modelos de embeddings
- **Groq**: Por la API de inferencia r√°pida

---

## üìä Estad√≠sticas del Proyecto

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.1+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen.svg)

---

<div align="center">

**‚≠ê Si este proyecto te ha sido √∫til, ¬°no olvides darle una estrella! ‚≠ê**

</div>