# ğŸ§  RAG Expert Assistant

**RAG Expert Assistant** es una aplicaciÃ³n avanzada construida con arquitectura modular en Python que convierte documentos en asistentes expertos usando **Retrieval-Augmented Generation (RAG)**. Carga documentos de cualquier tema (PDF, TXT, DOCX, etc.) y obtÃ©n respuestas contextualizadas como si interactuaras con un experto en el contenido.

---

## ğŸš€ CaracterÃ­sticas Principales

- ğŸ“„ **Carga mÃºltiple de documentos** (PDF, TXT, DOCX, MD, etc.)
- ğŸ“¦ **Almacenamiento vectorial inteligente** con ChromaDB
- ğŸ§  **Embeddings optimizados** con modelos Hugging Face
- ğŸ” **RecuperaciÃ³n hÃ­brida avanzada**: BM25 + Embeddings vectoriales
- ğŸ’¬ **Chat interactivo contextual** con IA conversacional
- ğŸ§± **Arquitectura modular escalable** con componentes desacoplados
- ğŸ“Š **EstadÃ­sticas en tiempo real** de documentos y embeddings
- ğŸ›ï¸ **Interfaz moderna** con Streamlit y componentes modulares
- ğŸ”§ **GestiÃ³n inteligente de memoria** y optimizaciÃ³n de recursos

---

## ğŸ—ï¸ Arquitectura del Sistema

### Componentes Principales

- **`UserInterface`**: Orquestador principal de la aplicaciÃ³n
- **`VectorStoreManager`**: GestiÃ³n del almacenamiento vectorial
- **`FileManager`**: Procesamiento y carga de documentos
- **`DocumentUI`**: Interfaz de gestiÃ³n de documentos
- **`DocumentDB`**: Persistencia de metadatos con SQLite
- **`RAGSystem`**: Motor principal de recuperaciÃ³n y generaciÃ³n

### MÃ³dulos UI Especializados

```
ui_components/
â”œâ”€â”€ model_manager.py          # GestiÃ³n de modelos de embeddings
â”œâ”€â”€ sidebar.py                # NavegaciÃ³n y configuraciÃ³n
â”œâ”€â”€ file_upload.py            # Carga de archivos avanzada
â”œâ”€â”€ search_interface.py       # BÃºsqueda semÃ¡ntica y por palabras clave
â””â”€â”€ LLM/
    â””â”€â”€ llm_interface.py      # Chat inteligente contextual
```

---

## ğŸ› ï¸ Stack TecnolÃ³gico

| CategorÃ­a | TecnologÃ­a | PropÃ³sito |
|-----------|------------|-----------|
| **Frontend** | Streamlit | Interfaz web interactiva |
| **Base de Datos Vectorial** | ChromaDB | Almacenamiento de embeddings |
| **Procesamiento LLM** | LangChain + Groq | Cadenas de recuperaciÃ³n y generaciÃ³n |
| **Embeddings** | Hugging Face Transformers | Modelos de embeddings semÃ¡nticos |
| **BÃºsqueda HÃ­brida** | BM25 + Vector Search | RecuperaciÃ³n precisa de contexto |
| **Base de Datos** | SQLite | Metadatos y estado de documentos |
| **Procesamiento** | PyPDF2, python-docx | ExtracciÃ³n de texto de documentos |

---

## ğŸ“‹ Requisitos del Sistema

- **Python**: 3.9+ (recomendado 3.10+)
- **RAM**: MÃ­nimo 4GB (recomendado 8GB+)
- **GPU**: Opcional (acelera embeddings)
- **Espacio**: 2GB+ libre para modelos y datos

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/tuusuario/demoSinesis.git
cd demoSinesis
```

### 2. Crear Entorno Virtual

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar Variables de Entorno

Crea un archivo `.env` en la raÃ­z del proyecto:

```env
# API Keys (elige uno o varios)
GROQ_API_KEY=tu_clave_groq_aqui
OPENAI_API_KEY=tu_clave_openai_aqui

# ConfiguraciÃ³n de Base de Datos
CHROMA_DB_PATH=BD/chroma_db_dir
SQLITE_DB_PATH=BD/documents.db

# ConfiguraciÃ³n de Modelos
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
LLM_MODEL=llama3-8b-8192
LLM_TEMPERATURE=0.7
```

### 5. Crear Estructura de Directorios

```bash
mkdir -p BD/chroma_db_dir
mkdir -p temp_uploads
```

---

## ğŸ¯ Uso de la AplicaciÃ³n

### Iniciar la AplicaciÃ³n

```bash
streamlit run main.py
```

La aplicaciÃ³n se abrirÃ¡ en `http://localhost:8501`

### Flujo de Trabajo

1. **ğŸ“¤ Carga de Documentos**
   - Usa el panel lateral para subir archivos
   - Soporta PDF, TXT, DOCX, MD
   - Visualiza el progreso de procesamiento

2. **ğŸ” Procesamiento Inteligente**
   - El sistema genera embeddings automÃ¡ticamente
   - Crea Ã­ndices vectoriales y BM25
   - Almacena metadatos en SQLite

3. **ğŸ’¬ Chat Contextual**
   - Haz preguntas en lenguaje natural
   - ObtÃ©n respuestas basadas en tus documentos
   - Ve las fuentes utilizadas para cada respuesta

4. **ğŸ“Š Monitoreo**
   - EstadÃ­sticas de documentos cargados
   - MÃ©tricas de rendimiento
   - Estado del sistema vectorial

---

## ğŸ“ Estructura del Proyecto

```
rag-expert-assistant/
â”œâ”€â”€ main.py                     # ğŸš€ Punto de entrada principal
â”œâ”€â”€ ui.py                       # ğŸ–¥ï¸ Interfaz principal refactorizada
â”œâ”€â”€ rag.py                      # ğŸ§  Motor RAG principal
â”œâ”€â”€ config.py                   # âš™ï¸ Configuraciones globales
â”œâ”€â”€ requirements.txt            # ğŸ“¦ Dependencias Python
â”œâ”€â”€ .env                        # ğŸ” Variables de entorno
â”œâ”€â”€ README.md                   # ğŸ“– DocumentaciÃ³n
â”‚
â”œâ”€â”€ BD/                         # ğŸ’¾ Bases de datos
â”‚   â”œâ”€â”€ chroma_db_dir/         # Almacenamiento vectorial
â”‚   â””â”€â”€ documents.db           # Metadatos SQLite
â”‚
â”œâ”€â”€ temp_uploads/              # ğŸ“ Archivos temporales
â”‚
â”œâ”€â”€ ui_components/             # ğŸ§© Componentes modulares
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_manager.py       # GestiÃ³n de modelos
â”‚   â”œâ”€â”€ sidebar.py             # Barra lateral
â”‚   â”œâ”€â”€ file_upload.py         # Carga de archivos
â”‚   â”œâ”€â”€ document_processor.py  # Procesamiento de documentos
â”‚   â”œâ”€â”€ search_interface.py    # Interfaz de bÃºsqueda
â”‚   â””â”€â”€ LLM/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ llm_interface.py   # Chat con IA
â”‚
â”œâ”€â”€ file_manager.py            # ğŸ“‚ GestiÃ³n de archivos
â”œâ”€â”€ document_ui.py             # ğŸ“„ UI de documentos
â”œâ”€â”€ document_processing.py     # ğŸ”„ Procesamiento de documentos
â”œâ”€â”€ document_db.py             # ğŸ—„ï¸ Base de datos de documentos
â””â”€â”€ vector_store.py            # ğŸ” GestiÃ³n del almacÃ©n vectorial
```

---

## ğŸª Casos de Uso

### ğŸ¢ Empresarial
- **CapacitaciÃ³n interna**: Manuales tÃ©cnicos y polÃ­ticas
- **Soporte tÃ©cnico**: Bases de conocimiento inteligentes
- **ConsultorÃ­a**: AnÃ¡lisis rÃ¡pido de documentos contractuales

### ğŸ“ AcadÃ©mico/InvestigaciÃ³n
- **InvestigaciÃ³n cientÃ­fica**: AnÃ¡lisis de papers y publicaciones
- **Estudios jurÃ­dicos**: Consultas a leyes y reglamentos
- **AnÃ¡lisis documental**: Procesamiento de grandes volÃºmenes de texto

### ğŸ‘¨â€ğŸ’¼ Personal/Profesional
- **GestiÃ³n del conocimiento**: OrganizaciÃ³n de documentos personales
- **Aprendizaje**: InteracciÃ³n con libros y materiales de estudio
- **AnÃ¡lisis de contenido**: ExtracciÃ³n de insights de documentos

---

## âš¡ CaracterÃ­sticas Avanzadas

### ğŸ” BÃºsqueda HÃ­brida
- **BÃºsqueda vectorial**: Similitud semÃ¡ntica avanzada
- **BM25**: BÃºsqueda por palabras clave tradicional
- **Ensemble**: CombinaciÃ³n ponderada de ambos mÃ©todos

### ğŸ§  IA Conversacional
- **Contexto dinÃ¡mico**: Respuestas basadas en documentos cargados
- **Fuentes citadas**: Transparencia en las respuestas
- **Memoria de conversaciÃ³n**: Mantiene el contexto del chat

### ğŸ“Š AnalÃ­ticas
- **MÃ©tricas de rendimiento**: Tiempo de respuesta y precisiÃ³n
- **EstadÃ­sticas de uso**: Documentos mÃ¡s consultados
- **Monitoreo de recursos**: Uso de memoria y CPU

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### PersonalizaciÃ³n de Modelos

```python
# En config.py
EMBEDDING_CONFIG = {
    "model_name": "sentence-transformers/all-MiniLM-L6-v2",
    "device": "cuda" if torch.cuda.is_available() else "cpu",
    "normalize_embeddings": True
}

LLM_CONFIG = {
    "model": "llama3-8b-8192",  # o "gpt-3.5-turbo"
    "temperature": 0.7,
    "max_tokens": 1024,
    "top_p": 0.9
}
```

### OptimizaciÃ³n de Rendimiento

```python
# ConfiguraciÃ³n para documentos grandes
CHUNK_CONFIG = {
    "chunk_size": 1000,
    "chunk_overlap": 200,
    "separators": ["\n\n", "\n", ". ", " "]
}

# ConfiguraciÃ³n de recuperaciÃ³n
RETRIEVAL_CONFIG = {
    "vector_k": 8,
    "bm25_k": 8,
    "score_threshold": 0.3,
    "ensemble_weights": [0.6, 0.4]  # [vectorial, bm25]
}
```

---

## ğŸ› SoluciÃ³n de Problemas

### Errores Comunes

#### `ModuleNotFoundError: No module named 'rag'`
```bash
# Verificar estructura de directorios
# Asegurar que existe ui_components/__init__.py
touch ui_components/__init__.py
touch ui_components/LLM/__init__.py
```

#### `ChromaDB collection not found`
```bash
# Limpiar y recrear base vectorial
rm -rf BD/chroma_db_dir
# Recargar documentos en la aplicaciÃ³n
```

#### `GROQ_API_KEY not found`
```bash
# Verificar archivo .env
echo "GROQ_API_KEY=tu_clave_aqui" >> .env
```

### Logs y DepuraciÃ³n

La aplicaciÃ³n genera logs detallados. Para habilitarlos:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## ğŸš§ Roadmap y Mejoras Futuras

### VersiÃ³n 2.0
- [ ] **Multi-usuario**: Sesiones separadas por usuario
- [ ] **API REST**: IntegraciÃ³n externa completa
- [ ] **Historial persistente**: Conversaciones guardadas
- [ ] **AnÃ¡lisis avanzado**: Dashboard de mÃ©tricas

### VersiÃ³n 2.1
- [ ] **Modelos locales**: Soporte para LLMs sin internet
- [ ] **VectorizaciÃ³n incremental**: ActualizaciÃ³n sin recrear
- [ ] **IntegraciÃ³n cloud**: AWS, GCP, Azure
- [ ] **Plugin system**: Extensiones personalizadas

---

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Por favor:

1. Fork del repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

### GuÃ­as de ContribuciÃ³n
- Seguir PEP 8 para estilo de cÃ³digo
- Agregar docstrings a funciones nuevas
- Incluir tests para nuevas funcionalidades
- Actualizar documentaciÃ³n cuando sea necesario

---

## ğŸ“ˆ MÃ©tricas y Rendimiento

### Benchmarks TÃ­picos
- **Carga de documento PDF (10 pÃ¡ginas)**: ~15-30 segundos
- **GeneraciÃ³n de respuesta**: ~2-5 segundos
- **BÃºsqueda en 100 documentos**: ~200-500ms
- **Uso de memoria**: ~500MB-2GB (dependiendo del corpus)

### Optimizaciones Implementadas
- âœ… Cache de embeddings con LRU
- âœ… Lazy loading de modelos
- âœ… Batch processing para documentos mÃºltiples
- âœ… CompresiÃ³n de vectores con cuantizaciÃ³n

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la **Licencia MIT**. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

```
MIT License

Copyright (c) 2025 RAG Expert Assistant

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...
```

---

## ğŸ‘¨â€ğŸ’» Autor

**Alex** ğŸš€  
ğŸ“ **Campeche, MÃ©xico**  
ğŸ› ï¸ **Consultor TI | Especialista en Python & AI**  

### ConÃ©ctame
- ğŸ“§ Email: [eric.vazquez.condor@gmail.com](mailto:eric.vazquez.condor@gmail.com)
- ğŸ’¼ LinkedIn: [Eric Vazquez](https://www.linkedin.com/in/eric-alejandro-vazquez-gongora-49342b148/l)
- ğŸ™ GitHub: [AlexVitesse](https://github.com/AlexVitesse)

---

## ğŸ™ Agradecimientos

- **LangChain**: Por el framework de LLM
- **ChromaDB**: Por la base de datos vectorial
- **Streamlit**: Por la interfaz web intuitiva  
- **Hugging Face**: Por los modelos de embeddings
- **Groq**: Por la API de inferencia rÃ¡pida

---

## ğŸ“Š EstadÃ­sticas del Proyecto

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.1+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Status](https://img.shields.io/badge/Status-Active-brightgreen.svg)

---

<div align="center">

**â­ Si este proyecto te ha sido Ãºtil, Â¡no olvides darle una estrella! â­**

</div>